{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c8589e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.2+cu102'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41d35484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc#my texts\n",
    "\n",
    "test = pd.read_csv('data/HeadHunter_test_prep.csv')\n",
    "train = pd.read_csv('data/HeadHunter_train_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2981798",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target']\n",
    "y_tr = []\n",
    "for labels in list(y_train):\n",
    "    y_tr.append([int(j) for j in labels.split(',')])\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_multilabels = mlb.fit_transform(list(y_tr))\n",
    "\n",
    "df_ = pd.DataFrame(list(train['review_id']), columns = ['review_id'])\n",
    "df_['comment_text'] = train['positive'] + ' | ' + train['negative']#train['position'] + ' | ' + \n",
    "for i in ['0', '1', '2', '3', '4', '5', '6', '7', '8']:\n",
    "    df_[i] = y_train_multilabels[:, int(i)].astype(float)\n",
    "df_train = df_\n",
    "\n",
    "df_ = pd.DataFrame(list(test['review_id']), columns = ['review_id'])\n",
    "df_['comment_text'] = test['positive'] + ' | ' + test['negative']#test['position'] + ' | ' + \n",
    "df_test = df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d76283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['salary_rating', 'team_rating', 'managment_rating', 'career_rating', 'workplace_rating',\n",
    "            'rest_recovery_rating', 'population', 'position_len', 'negative_len',\n",
    "            'positive_len', 'edit_d', 'pos_the_longest_token',\n",
    "            'neg_the_longest_token', 'pos_lex_div', 'neg_lex_div']\n",
    "train_features = train[features].values.astype(np.float32)\n",
    "# test# train_features = train[features].values.astype(np.float32)\n",
    "test_features = test[features].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b3c8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "train_features = ss.fit_transform(train_features)\n",
    "test_features = ss.fit_transform(test_features)\n",
    "df_train.fillna('', inplace=True)\n",
    "df_test.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78cadf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b00fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "  def __init__(self, texts, features, targets, tokenizer, max_len=None):\n",
    "    self.texts = texts\n",
    "    self.features = features\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    target = self.targets[idx]\n",
    "    features = self.features[idx]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'text': text,\n",
    "      'features' : features,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target)#torch.tensor(target, dtype=torch.long)\n",
    "    }\n",
    "    \n",
    "from transformers import AutoConfig, AutoModel\n",
    "import torch\n",
    "\n",
    "from transformers import BertModel\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, bert_model, cfg, features_len, n_out):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.features_len = features_len\n",
    "        self.out_features = self.bert_model.config.hidden_size + features_len\n",
    "        ### New layers:\n",
    "        self.classifier = nn.Sequential(\n",
    "          nn.Linear(self.out_features, 64),  \n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, n_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, features):\n",
    "        hidden_states = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        cls_embeds = hidden_states.last_hidden_state[:, 0, :] \n",
    "        concat = torch.cat((cls_embeds, features), dim=-1)\n",
    "        linear_output = self.classifier(concat)\n",
    "        return linear_output\n",
    "\n",
    "class BertClassifier:\n",
    "    def __init__(self, model_path, features_len, tokenizer_path, MAX_len = 128, n_classes=2, epochs=1, model_save_path='/content/bert.pt'):\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        self.bert_model = AutoModel.from_pretrained(model_path, config=self.config)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_save_path=model_save_path\n",
    "        self.max_len = MAX_len\n",
    "        self.epochs = epochs\n",
    "        self.model = CustomBERTModel(bert_model=self.bert_model, cfg=self.config, features_len=features_len, n_out=n_classes)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def preparation(self, X_train, X_train_features, y_train, X_valid, X_valid_features, y_valid):\n",
    "        # create datasets\n",
    "        self.train_set = CustomDataset(X_train, X_train_features, y_train, self.tokenizer, max_len=self.max_len)\n",
    "        self.valid_set = CustomDataset(X_valid, X_valid_features, y_valid, self.tokenizer, max_len=self.max_len)\n",
    "\n",
    "        # create data loaders\n",
    "        self.train_loader = DataLoader(self.train_set, batch_size=2, shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_set, batch_size=2, shuffle=True)\n",
    "\n",
    "        # helpers initialization\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=len(self.train_loader) * self.epochs\n",
    "            )\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss().to(self.device)\n",
    "\n",
    "    def fit(self):\n",
    "        self.model = self.model.train()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "        preds, true = None, None\n",
    "\n",
    "        for data in tqdm(self.train_loader):\n",
    "            input_ids = data[\"input_ids\"].to(self.device)\n",
    "            features = data[\"features\"].to(self.device)\n",
    "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "            targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                features=features\n",
    "                )\n",
    "            \n",
    "            if preds is None:\n",
    "                preds = outputs\n",
    "                true = targets\n",
    "            else:\n",
    "                preds = torch.vstack([preds, outputs])\n",
    "                true = torch.vstack([true, targets])\n",
    "            \n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        train_acc = f1_score(preds.cpu() > 0.5, true.cpu(), average='samples')\n",
    "        train_loss = np.mean(losses)\n",
    "        return train_acc, train_loss\n",
    "    \n",
    "    def eval(self):\n",
    "        self.model = self.model.eval()\n",
    "        losses = []\n",
    "        preds, true = None, None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(self.valid_loader):\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                features = data[\"features\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    features=features\n",
    "                    )\n",
    "\n",
    "                if preds is None:\n",
    "                    preds = outputs\n",
    "                    true = targets\n",
    "                else:\n",
    "                    preds = torch.vstack([preds, outputs])\n",
    "                    true = torch.vstack([true, targets])\n",
    "                \n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                losses.append(loss.item())\n",
    "        \n",
    "        val_acc = f1_score(preds.cpu() > 0.5, true.cpu(), average='samples')\n",
    "        val_loss = np.mean(losses)\n",
    "        return val_acc, val_loss\n",
    "    \n",
    "    def train(self):\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
    "            train_acc, train_loss = self.fit()\n",
    "            print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "            val_acc, val_loss = self.eval()\n",
    "            print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "            print('-' * 10)\n",
    "            model_save_path_with_metrics = self.model_save_path.split('.')[0] + '_' + str(round(train_acc, 3)) + '_' + str(round(val_acc, 3)) + '.pt'\n",
    "            torch.save(self.model, model_save_path_with_metrics)\n",
    "            if val_acc > best_accuracy:\n",
    "                torch.save(self.model, self.model_save_path)\n",
    "                best_accuracy = val_acc\n",
    "\n",
    "        self.model = torch.load(self.model_save_path)\n",
    "    \n",
    "    def predict(self, text, features):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        out = {\n",
    "              'text': text,\n",
    "              'input_ids': encoding['input_ids'].flatten(),\n",
    "              'attention_mask': encoding['attention_mask'].flatten()\n",
    "          }\n",
    "        \n",
    "        input_ids = out[\"input_ids\"].to(self.device)\n",
    "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids.unsqueeze(0),\n",
    "            attention_mask=attention_mask.unsqueeze(0),\n",
    "            features=features\n",
    "        )\n",
    "        \n",
    "        prediction = torch.nn.functional.sigmoid(outputs).cpu().detach().numpy()[0]\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65cfb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_len = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c94a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "classifier = BertClassifier(\n",
    "        model_path = 'cointegrated/rubert-tiny',#модель побольше - 'blanchefort/rubert-base-cased-sentiment',\n",
    "        tokenizer_path = 'cointegrated/rubert-tiny',#модель побольше -'blanchefort/rubert-base-cased-sentiment',\n",
    "        MAX_len = 128,    \n",
    "        n_classes=9,\n",
    "        epochs=10,\n",
    "        model_save_path='rubert_tiny.pt',\n",
    "        features_len = features_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e0918d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid, f_train, f_valid = train_test_split(df_train, train_features, test_size=0.33, random_state=42)\n",
    "    #_, data_valid, _, f_valid = train_test_split(data_valid, f_valid, test_size=0.01, random_state=42)\n",
    "\n",
    "X_train = list(data_train['comment_text'])\n",
    "y_train = list(data_train[[str(i) for i in range(9)]].values)\n",
    "\n",
    "X_valid = list(data_valid['comment_text'])\n",
    "y_valid = list(data_valid[[str(i) for i in range(9)]].values)\n",
    "\n",
    "classifier.preparation(\n",
    "    X_train=X_train,\n",
    "    X_train_features=f_train,\n",
    "    y_train=y_train,\n",
    "    X_valid=X_valid,\n",
    "    X_valid_features=f_valid,\n",
    "    y_valid=y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fa921e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(classifier.model, 'tiny_bert_one_more_attempt.pt')\n",
    "# classifier.model = torch.load('tiny_bert_one_more_attempt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ef97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f8dc83e09849fe9e01ed4782d12160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17043 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.15114413722924147 accuracy 0.7499970662442059\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cc5661790c49ddaa6ba7927a41fbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.1363672128138708 accuracy 0.7885447687115346\n",
      "----------\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14c18ab22a143cf9f3f083591faf831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17043 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.12967096366692843 accuracy 0.795610123413327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b3fdad8ece446d91df775a03a0863d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.13952822241083335 accuracy 0.7931606114750843\n",
      "----------\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946fd7e62e7e4d2cbc624565bb5c7554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17043 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.12073350756351273 accuracy 0.811374171213988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf70696a52b241c3b2d0ae85c235f977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.model = torch.load('add_position_rubert_tiny_0.816_0.97.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "predictions = np.stack([classifier.predict(txt, f.unsqueeze(0)) for txt, f in tqdm(list(zip(X_train, torch.tensor(f_train).to(device))))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7da141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(thresholds):\n",
    "    y_true = y_train\n",
    "    y_pred = np.stack([predictions[::, i] > thresholds[i] for i in range(9)]).T.astype(int)\n",
    "    return f1_score(y_pred, y_true, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    thresholds = {0: trial.suggest_float('0', 0.0, 1.0),\n",
    "     1: trial.suggest_float('1', 0.0, 1.0),\n",
    "     2: trial.suggest_float('2', 0.0, 1.0),\n",
    "     3: trial.suggest_float('3', 0.0, 1.0),\n",
    "     4: trial.suggest_float('4', 0.0, 1.0),\n",
    "     5: trial.suggest_float('5', 0.0, 1.0),\n",
    "     6: trial.suggest_float('6', 0.0, 1.0),\n",
    "     7: trial.suggest_float('7', 0.0, 1.0),\n",
    "     8: trial.suggest_float('8', 0.0, 1.0),}\n",
    "    return get_score(thresholds)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d955537",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbe498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds['0'] = 0.05\n",
    "# thresholds['8'] = 0.195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_records([thresholds]).to_csv('best_tresholds_tiny_rubert_128_0_0.05_dotrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605832da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_probs = np.stack([classifier.predict(txt, f.unsqueeze(0)) for txt, f in tqdm(list(zip(df_test.comment_text, torch.tensor(test_features).to(device))))])\n",
    "test_predictions = np.stack([test_predictions_probs[::, i] > thresholds[str(i)] for i in range(9)]).T.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70986f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([df_test[['review_id']], pd.DataFrame(test_predictions_probs, columns = ['0', '1', '2', '3', '4', '5', '6', '7', '8'])], axis=1).to_csv('rubert_tiny_128_0.823_0.879_0_0.05_probs_test_dotrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a757db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "y_pred_test = test_predictions\n",
    "y_pred_test_mod = deepcopy(y_pred_test)\n",
    "for i in range(len(y_pred_test_mod)):\n",
    "    if np.sum(y_pred_test_mod[i]) == 0:\n",
    "        y_pred_test_mod[i][0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_submit = df_test[['review_id']]#$pd.read_csv('HeadHunter_sample_submit.csv')\n",
    "\n",
    "target_column = list()\n",
    "for i in range(len(y_pred_test_mod)):\n",
    "    target_column.append(','.join(map(str, np.where(y_pred_test_mod[i])[0])))\n",
    "\n",
    "example_submit['target'] = target_column\n",
    "example_submit.to_csv(\"submits/rubert_tiny_128_128_0.825_0.97.csv\", index=False)\n",
    "#\"rubert_tiny_128_0.823_0.879_0_0.05_test_labels_dotrain.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82b0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
